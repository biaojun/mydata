# å¤š vLLM å®ä¾‹å¹¶å‘æ¨ç†æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ 4 ä¸ª vLLM å®ä¾‹è¿›è¡Œå¹¶å‘æ¨ç†ï¼Œå……åˆ†åˆ©ç”¨ 8 å¼  GPUï¼ˆ0-7ï¼‰ã€‚

## æ¶æ„è¯´æ˜

```
GPU åˆ†é…:
â”œâ”€â”€ vLLM å®ä¾‹ 1 (ç«¯å£ 8001): GPU 0, 1
â”œâ”€â”€ vLLM å®ä¾‹ 2 (ç«¯å£ 8002): GPU 2, 3
â”œâ”€â”€ vLLM å®ä¾‹ 3 (ç«¯å£ 8003): GPU 4, 5
â””â”€â”€ vLLM å®ä¾‹ 4 (ç«¯å£ 8004): GPU 6, 7

ä»»åŠ¡åˆ†å‘:
Pipeline â†’ MultiVLLMClient â†’ è½®è¯¢åˆ†å‘åˆ° 4 ä¸ªå®ä¾‹
           (è´Ÿè½½å‡è¡¡)
```

## å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: å¯åŠ¨ 4 ä¸ª vLLM å®ä¾‹

```bash
cd /Users/orangezhi/Desktop/mydata
chmod +x start_vllm.sh stop_vllm.sh check_vllm.sh run_multi_vllm.sh
./start_vllm.sh
```

ç­‰å¾…çº¦ 30-60 ç§’è®©æ‰€æœ‰å®ä¾‹å®Œæˆæ¨¡å‹åŠ è½½ã€‚

### æ­¥éª¤ 2: æ£€æŸ¥å®ä¾‹çŠ¶æ€

```bash
./check_vllm.sh
```

åº”è¯¥çœ‹åˆ°æ‰€æœ‰ 4 ä¸ªç«¯å£éƒ½åœ¨ç›‘å¬ï¼Œä¸” API å¯è®¿é—®ã€‚

### æ­¥éª¤ 3: è¿è¡Œå¹¶å‘æ¨ç†

```bash
./run_multi_vllm.sh
```

æˆ–æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
export USE_MULTI_VLLM=true
export VLLM_PORTS="8001,8002,8003,8004"
export INPUT_JSONL="data/tasks.jsonl"
export OUTPUT_DIR="outputs"

python -m analyze.pipeline
```

### æ­¥éª¤ 4: æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹æ‰€æœ‰å®ä¾‹æ—¥å¿—
tail -f vllm_logs/vllm_*.log

# æŸ¥çœ‹å•ä¸ªå®ä¾‹æ—¥å¿—
tail -f vllm_logs/vllm_01.log
```

### æ­¥éª¤ 5: åœæ­¢å®ä¾‹

```bash
./stop_vllm.sh
```

## å·¥ä½œåŸç†

### 1. å¤šå®ä¾‹å®¢æˆ·ç«¯ (`MultiVLLMClient`)

`analyze/multi_vllm.py` å®ç°äº†ä¸€ä¸ªæ™ºèƒ½å®¢æˆ·ç«¯ï¼š

- **è½®è¯¢è´Ÿè½½å‡è¡¡**: ä»»åŠ¡è½®æµåˆ†é…åˆ° 4 ä¸ªå®ä¾‹
- **å…¼å®¹æ¥å£**: ä¸ `OpenAI` client æ¥å£ä¸€è‡´
- **çº¿ç¨‹å®‰å…¨**: ä½¿ç”¨é”ä¿æŠ¤å¹¶å‘è®¿é—®

### 2. å¹¶å‘æ¨ç†æµç¨‹

```python
# 100 ä¸ªä»»åŠ¡å¹¶å‘å¤„ç†ç¤ºä¾‹
ä»»åŠ¡  1 â†’ vLLM å®ä¾‹ 1 (8001)  â”
ä»»åŠ¡  2 â†’ vLLM å®ä¾‹ 2 (8002)  â”‚ å¹¶å‘
ä»»åŠ¡  3 â†’ vLLM å®ä¾‹ 3 (8003)  â”‚ å¤„ç†
ä»»åŠ¡  4 â†’ vLLM å®ä¾‹ 4 (8004)  â”˜
ä»»åŠ¡  5 â†’ vLLM å®ä¾‹ 1 (8001)  â† è½®è¯¢
ä»»åŠ¡  6 â†’ vLLM å®ä¾‹ 2 (8002)
...
```

### 3. è¿›åº¦æ˜¾ç¤º

è¿è¡Œæ—¶ä¼šçœ‹åˆ°ï¼š

```
============================================================
ğŸš€ å¼€å§‹è¿è¡Œ 1vN ä»£ç è´¨é‡åˆ†æ Pipeline
============================================================
ğŸš€ å¯ç”¨å¤š vLLM å®ä¾‹å¹¶å‘æ¨¡å¼
   ä½¿ç”¨ 4 ä¸ª vLLM å®ä¾‹: ç«¯å£ [8001, 8002, 8003, 8004]

ğŸ“– [æ­¥éª¤ 1/6] è¯»å–ä»»åŠ¡æ•°æ®...
   âœ“ æˆåŠŸè¯»å– 100 ä¸ªä»»åŠ¡

ğŸ¤– [æ­¥éª¤ 2/6] è°ƒç”¨ LLM åˆ†æä»»åŠ¡...
åˆ†æä»»åŠ¡:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 25/100 [01:15<03:45, æˆåŠŸ=24, å¤±è´¥=1]
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è°ƒæ•´å¹¶å‘æ•°

é»˜è®¤æƒ…å†µä¸‹ï¼Œä»»åŠ¡æ˜¯é¡ºåºå¤„ç†çš„ï¼ˆä¸€ä¸ªæ¥ä¸€ä¸ªï¼‰ã€‚è™½ç„¶ 4 ä¸ª vLLM å®ä¾‹åœ¨åå°å¹¶å‘ï¼Œä½†å‰ç«¯ä»æ˜¯ä¸²è¡Œã€‚

å¦‚æœæƒ³è¦æ›´é«˜ååé‡ï¼Œå¯ä»¥ä¿®æ”¹ `per_task.py` ä½¿ç”¨çœŸæ­£çš„å¹¶å‘ï¼š

```python
from concurrent.futures import ThreadPoolExecutor

def analyze_tasks_concurrent(tasks, model, max_workers=4):
    """ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æä»»åŠ¡ã€‚"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(
            lambda t: analyze_task(t, model),
            tasks
        ))
    return [r for r in results if r is not None]
```

### 2. è°ƒæ•´ vLLM å‚æ•°

æ ¹æ®æ‚¨çš„éœ€æ±‚è°ƒæ•´ `start_vllm.sh` ä¸­çš„å‚æ•°ï¼š

- `--max-num-seqs`: å•å®ä¾‹å¹¶å‘è¯·æ±‚æ•°ï¼ˆå½“å‰ 6ï¼‰
- `--gpu-memory-utilization`: GPU æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆå½“å‰ 0.96ï¼‰
- `--max-model-len`: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå½“å‰ 8192ï¼‰

### 3. ç›‘æ§ GPU ä½¿ç”¨

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# æŸ¥çœ‹ç‰¹å®š GPU
nvidia-smi -i 0,1,2,3,4,5,6,7
```

## ç¯å¢ƒå˜é‡å‚è€ƒ

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `USE_MULTI_VLLM` | `true` | æ˜¯å¦å¯ç”¨å¤šå®ä¾‹æ¨¡å¼ |
| `VLLM_PORTS` | `8001,8002,8003,8004` | vLLM å®ä¾‹ç«¯å£åˆ—è¡¨ |
| `VLLM_HOST` | `localhost` | vLLM å®ä¾‹ä¸»æœºåœ°å€ |
| `INPUT_JSONL` | `data/tasks.jsonl` | è¾“å…¥ä»»åŠ¡æ–‡ä»¶ |
| `OUTPUT_DIR` | `outputs` | è¾“å‡ºç›®å½• |
| `FONT_PATH` | - | ä¸­æ–‡å­—ä½“è·¯å¾„ï¼ˆè¯äº‘ï¼‰ |

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: ç«¯å£è¢«å ç”¨

```bash
# æŸ¥çœ‹ç«¯å£å ç”¨
lsof -i :8001
lsof -i :8002
lsof -i :8003
lsof -i :8004

# æ€æ­»å ç”¨è¿›ç¨‹
kill -9 <PID>
```

### é—®é¢˜ 2: GPU å†…å­˜ä¸è¶³

```bash
# æŸ¥çœ‹ GPU æ˜¾å­˜
nvidia-smi

# å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå‡å°‘å®ä¾‹æ•°é‡æˆ–é™ä½ gpu-memory-utilization
# ç¼–è¾‘ start_vllm.shï¼Œå°† 0.96 æ”¹ä¸º 0.90
```

### é—®é¢˜ 3: å®ä¾‹æ— å“åº”

```bash
# æŸ¥çœ‹æ—¥å¿—
tail -100 vllm_logs/vllm_01.log

# é‡å¯æ‰€æœ‰å®ä¾‹
./stop_vllm.sh
./start_vllm.sh
```

### é—®é¢˜ 4: è¿æ¥è¶…æ—¶

æ£€æŸ¥é˜²ç«å¢™è®¾ç½®ï¼Œç¡®ä¿ç«¯å£ 8001-8004 å¯è®¿é—®ã€‚

## ä»£ç ç¤ºä¾‹

### åœ¨ Python ä¸­ä½¿ç”¨

```python
from analyze import create_multi_vllm_client, run_pipeline

# åˆ›å»ºå¤šå®ä¾‹å®¢æˆ·ç«¯
client = create_multi_vllm_client(
    ports=[8001, 8002, 8003, 8004],
    host="localhost"
)

# è¿è¡Œ Pipeline
paths = run_pipeline(
    input_jsonl="data/tasks.jsonl",
    output_dir="outputs",
    client=client
)

print(f"ç»“æœ: {paths}")
```

### å•ç‹¬ä½¿ç”¨ MultiVLLMClient

```python
from analyze import MultiVLLMClient

# åˆå§‹åŒ–
client = MultiVLLMClient([
    "http://localhost:8001/v1",
    "http://localhost:8002/v1",
    "http://localhost:8003/v1",
    "http://localhost:8004/v1",
])

# è°ƒç”¨ï¼ˆè‡ªåŠ¨è´Ÿè½½å‡è¡¡ï¼‰
response = client.chat.completions.create(
    model="/var/shared/models/Qwen3-30B-A3B-Instruct-2507",
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7
)

print(response.choices[0].message.content)
```

## æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | å®ä¾‹æ•° | GPU æ•° | ç†è®ºååé‡ | æ¨èåœºæ™¯ |
|------|--------|--------|------------|----------|
| å•å®ä¾‹ | 1 | 2 | 1x | å°è§„æ¨¡ä»»åŠ¡ |
| åŒå®ä¾‹ | 2 | 4 | ~2x | ä¸­ç­‰è§„æ¨¡ |
| å››å®ä¾‹ | 4 | 8 | ~4x | å¤§è§„æ¨¡æ‰¹å¤„ç† â­ |

å®é™…ååé‡å–å†³äºï¼š
- æ¨¡å‹æ¨ç†é€Ÿåº¦
- ç½‘ç»œå»¶è¿Ÿ
- ä»»åŠ¡å¤æ‚åº¦
- GPU æ€§èƒ½

## æœ€ä½³å®è·µ

1. âœ… **å¯åŠ¨å‰æ£€æŸ¥**: ç¡®ä¿æ‰€æœ‰ GPU å¯ç”¨ä¸”æ˜¾å­˜å……è¶³
2. âœ… **é€æ­¥æµ‹è¯•**: å…ˆå¯åŠ¨ 1 ä¸ªå®ä¾‹æµ‹è¯•ï¼Œå†æ‰©å±•åˆ° 4 ä¸ª
3. âœ… **ç›‘æ§èµ„æº**: è¿è¡Œæ—¶ç›‘æ§ GPU å’Œå†…å­˜ä½¿ç”¨
4. âœ… **å®šæœŸæ¸…ç†**: å®ŒæˆååŠæ—¶åœæ­¢å®ä¾‹é‡Šæ”¾èµ„æº
5. âœ… **ä¿å­˜æ—¥å¿—**: ä¿ç•™æ—¥å¿—æ–‡ä»¶ç”¨äºè°ƒè¯•å’Œæ€§èƒ½åˆ†æ

## è¿›é˜¶ï¼šçœŸæ­£çš„å¹¶å‘å¤„ç†

å¦‚æœæƒ³è¦åŒæ—¶å‘ 4 ä¸ªå®ä¾‹å‘é€è¯·æ±‚ï¼ˆè€Œä¸æ˜¯è½®è¯¢ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨çº¿ç¨‹æ± ï¼š

```python
from concurrent.futures import ThreadPoolExecutor
from analyze import analyze_task

def analyze_tasks_parallel(tasks, model, max_workers=4):
    """çœŸæ­£å¹¶å‘åœ°åˆ†æä»»åŠ¡ã€‚"""
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            executor.submit(analyze_task, task, model): task
            for task in tasks
        }
        
        results = []
        for future in as_completed(futures):
            result = future.result()
            if result is not None:
                results.append(result)
        
        return results
```

è¿™æ ·å¯ä»¥åŒæ—¶å¤„ç† 4 ä¸ªä»»åŠ¡ï¼Œå……åˆ†åˆ©ç”¨ 4 ä¸ª vLLM å®ä¾‹çš„å¹¶å‘èƒ½åŠ›ï¼
