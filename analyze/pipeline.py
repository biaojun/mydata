"""ä¸€é”®è¿è¡Œ 1vN ä»£ç è´¨é‡åˆ†æçš„ Pipelineã€‚

æµç¨‹ï¼š
1) è¯»å– JSONLï¼ˆæ¯è¡Œå« task, good_code[list], bad_code[list]ï¼Œå¯é€‰ task_id/languageï¼‰
2) æ„é€  1vN Promptï¼Œè°ƒç”¨ LLMï¼ˆsend_vllmï¼‰å¹¶è§£æè¾“å‡º â†’ per_task.jsonl
3) èšåˆå¯¼å‡ºï¼šagg_dimension.csv ä¸ agg_keywords.csv
4) ç”Ÿæˆå…¨å±€å›¾è¡¨ï¼ˆé›·è¾¾/çƒ­åŠ›å›¾ï¼‰å¹¶ç”Ÿæˆ report.md

ä½¿ç”¨ï¼š
  ç›´æ¥æ‰§è¡Œï¼špython -m analyze.pipeline
  å¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–è·¯å¾„ï¼š
    INPUT_JSONL=data/tasks.jsonl OUTPUT_DIR=outputs python -m analyze.pipeline

æ³¨æ„ï¼šä½ éœ€è¦åœ¨ analyze/llm_runner.py ä¸­å®ç° send_vllm(prompt) æ‰èƒ½çœŸæ­£è°ƒç”¨æ¨¡å‹ã€‚
"""

from __future__ import annotations

import os
from typing import List

from .io_utils import read_tasks_jsonl, write_jsonl, ensure_dir
from .per_task import analyze_tasks
from .aggregate import export_aggregates
from .visualize import plot_global_radar, plot_global_heatmaps, plot_global_wordcloud
from .report import build_report_markdown


def run_pipeline(
    input_jsonl: str = "data/tasks.jsonl",
    output_dir: str = "outputs",
    client = None,
    show_progress: bool = True,
    use_concurrent: bool = True,
    max_workers: int = 4
) -> dict:
    """è¿è¡Œå®Œæ•´çš„ 1vN ä»£ç è´¨é‡åˆ†æ Pipelineã€‚
    
    Args:
        input_jsonl: è¾“å…¥ä»»åŠ¡çš„ JSONL æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        client: OpenAI client å®ä¾‹
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        use_concurrent: æ˜¯å¦ä½¿ç”¨å¹¶å‘å¤„ç†ï¼ˆå»ºè®®å¤šå®ä¾‹æ—¶å¯ç”¨ï¼‰
        max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®®ä¸ vLLM å®ä¾‹æ•°ç›¸åŒï¼‰
    
    Returns:
        åŒ…å«å„è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è¿è¡Œ 1vN ä»£ç è´¨é‡åˆ†æ Pipeline")
    print("=" * 60)
    
    # 1) è¯»å–ä»»åŠ¡
    print("\nğŸ“– [æ­¥éª¤ 1/6] è¯»å–ä»»åŠ¡æ•°æ®...")
    tasks = read_tasks_jsonl(input_jsonl)
    print(f"   âœ“ æˆåŠŸè¯»å– {len(tasks)} ä¸ªä»»åŠ¡")

    # 2) è°ƒç”¨ LLM åˆ†ææ¯ä»»åŠ¡
    print("\nğŸ¤– [æ­¥éª¤ 2/6] è°ƒç”¨ LLM åˆ†æä»»åŠ¡...")
    if use_concurrent:
        print(f"   ä½¿ç”¨å¹¶å‘æ¨¡å¼ï¼ˆ{max_workers} ä¸ªçº¿ç¨‹ï¼‰")
    results = analyze_tasks(
        tasks, 
        model=client, 
        show_progress=show_progress,
        use_concurrent=use_concurrent,
        max_workers=max_workers
    )
    print(f"   âœ“ æˆåŠŸåˆ†æ {len(results)} ä¸ªä»»åŠ¡")

    # 3) å†™ per_task.jsonl
    print("\nğŸ’¾ [æ­¥éª¤ 3/6] ä¿å­˜ä»»åŠ¡åˆ†æç»“æœ...")
    ensure_dir(output_dir)
    per_task_path = os.path.join(output_dir, "per_task.jsonl")
    write_jsonl(per_task_path, results)
    print(f"   âœ“ å·²ä¿å­˜åˆ°: {per_task_path}")

    # 4) èšåˆå¯¼å‡º CSV
    print("\nğŸ“Š [æ­¥éª¤ 4/6] èšåˆç»Ÿè®¡æ•°æ®...")
    dim_csv = os.path.join(output_dir, "agg_dimension.csv")
    kw_csv = os.path.join(output_dir, "agg_keywords.csv")
    export_aggregates(per_task_path, dim_csv, kw_csv)
    print(f"   âœ“ ç»´åº¦ç»Ÿè®¡: {dim_csv}")
    print(f"   âœ“ å…³é”®è¯ç»Ÿè®¡: {kw_csv}")

    # 5) ç”Ÿæˆå…¨å±€å›¾è¡¨
    print("\nğŸ“ˆ [æ­¥éª¤ 5/6] ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    figs_dir = os.path.join(output_dir, "figs")
    ensure_dir(figs_dir)
    
    radar_path = os.path.join(figs_dir, "global_radar.png")
    plot_global_radar(dim_csv, radar_path)
    print(f"   âœ“ é›·è¾¾å›¾: {radar_path}")
    
    heatmap_path = plot_global_heatmaps(dim_csv, figs_dir)
    print(f"   âœ“ çƒ­åŠ›å›¾: {heatmap_path}")
    
    # è¯»å– FONT_PATH ç¯å¢ƒå˜é‡ä»¥æ”¯æŒä¸­æ–‡å­—ä½“
    font_path = os.environ.get("FONT_PATH")
    try:
        wordcloud_path = os.path.join(figs_dir, "global_wordcloud.png")
        plot_global_wordcloud(kw_csv, wordcloud_path, font_path=font_path)
        print(f"   âœ“ è¯äº‘å›¾: {wordcloud_path}")
    except Exception as e:
        print(f"   âš  è·³è¿‡è¯äº‘ç”Ÿæˆ: {e}")
        wordcloud_path = ""

    # 6) ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ [æ­¥éª¤ 6/6] ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report_md = os.path.join(output_dir, "report.md")
    build_report_markdown(dim_csv, kw_csv, figs_dir, report_md)
    print(f"   âœ“ æŠ¥å‘Š: {report_md}")

    print("\n" + "=" * 60)
    print("âœ… Pipeline æ‰§è¡Œå®Œæˆï¼")
    print("=" * 60)

    return {
        "per_task": per_task_path,
        "agg_dimension": dim_csv,
        "agg_keywords": kw_csv,
        "radar": radar_path,
        "heatmap": heatmap_path,
        "report": report_md,
        "wordcloud": wordcloud_path,
    }


def main() -> int:
    input_jsonl = os.environ.get("INPUT_JSONL", "/home/liangjunbiao/data/processed_data.jsonl")
    output_dir = os.environ.get("OUTPUT_DIR", "outputs")
    
    # æ”¯æŒå¤š vLLM å®ä¾‹å¹¶å‘
    use_multi_vllm = os.environ.get("USE_MULTI_VLLM", "true").lower() in ("true", "1", "yes")
    # æ˜¯å¦å¯ç”¨çœŸæ­£çš„å¹¶å‘å¤„ç†ï¼ˆçº¿ç¨‹æ± ï¼‰
    use_concurrent = os.environ.get("USE_CONCURRENT", "true").lower() in ("true", "1", "yes")
    # å¹¶å‘çº¿ç¨‹æ•°
    max_workers = int(os.environ.get("MAX_WORKERS", "4"))
    
    if use_multi_vllm:
        print("ğŸš€ å¯ç”¨å¤š vLLM å®ä¾‹å¹¶å‘æ¨¡å¼")
        from .multi_vllm import create_multi_vllm_client, get_vllm_urls_from_env
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–ç«¯å£æˆ– URL
        vllm_ports_str = os.environ.get("VLLM_PORTS", "8001,8002,8003,8004")
        vllm_host = os.environ.get("VLLM_HOST", "localhost")
        
        ports = [int(p.strip()) for p in vllm_ports_str.split(",")]
        client = create_multi_vllm_client(ports=ports, host=vllm_host)
        
        print(f"   ä½¿ç”¨ {len(ports)} ä¸ª vLLM å®ä¾‹: ç«¯å£ {ports}")
        if use_concurrent:
            print(f"   çœŸæ­£å¹¶å‘æ¨¡å¼: {max_workers} ä¸ªçº¿ç¨‹åŒæ—¶å¤„ç†")
        else:
            print(f"   è½®è¯¢æ¨¡å¼: ä¸²è¡Œå¤„ç†ï¼ˆè½®æµä½¿ç”¨å®ä¾‹ï¼‰")
    else:
        print("ğŸ”§ ä½¿ç”¨å• vLLM å®ä¾‹æ¨¡å¼")
        from openai import OpenAI
        
        vllm_url = os.environ.get("VLLM_URL", "http://localhost:8000/v1")
        client = OpenAI(base_url=vllm_url, api_key="EMPTY")
        
        print(f"   vLLM URL: {vllm_url}")
        use_concurrent = False  # å•å®ä¾‹ä¸å»ºè®®å¹¶å‘

    paths = run_pipeline(
        input_jsonl=input_jsonl, 
        output_dir=output_dir, 
        client=client,
        use_concurrent=use_concurrent,
        max_workers=max_workers
    )
    
    print("\n" + "=" * 60)
    print("ğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
    print("=" * 60)
    for k, v in paths.items():
        print(f"  {k}: {v}")
    
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
