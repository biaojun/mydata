"""ä¸€é”®è¿è¡Œ 1vN ä»£ç è´¨é‡åˆ†æçš„ Pipelineã€‚

æµç¨‹ï¼š
1) è¯»å– JSONLï¼ˆæ¯è¡Œå« task, good_code[list], bad_code[list]ï¼Œå¯é€‰ task_id/languageï¼‰
2) æ„é€  1vN Promptï¼Œè°ƒç”¨ LLMï¼ˆsend_vllmï¼‰å¹¶è§£æè¾“å‡º â†’ per_task.jsonl
3) èšåˆå¯¼å‡ºï¼šagg_dimension.csv ä¸ agg_keywords.csv
4) ç”Ÿæˆå…¨å±€å›¾è¡¨ï¼ˆé›·è¾¾/çƒ­åŠ›å›¾ï¼‰å¹¶ç”Ÿæˆ report.md

ä½¿ç”¨ï¼š
  ç›´æ¥æ‰§è¡Œï¼špython -m analyze.pipeline
  å¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–è·¯å¾„ï¼š
    INPUT_JSONL=data/tasks.jsonl OUTPUT_DIR=outputs python -m analyze.pipeline

æ³¨æ„ï¼šä½ éœ€è¦åœ¨ analyze/llm_runner.py ä¸­å®ç° send_vllm(prompt) æ‰èƒ½çœŸæ­£è°ƒç”¨æ¨¡å‹ã€‚
"""

from __future__ import annotations

import os
from typing import List

from .io_utils import read_tasks_jsonl, write_jsonl, ensure_dir
from .per_task import analyze_tasks
from .aggregate import export_aggregates
from .visualize import plot_global_radar, plot_global_heatmaps, plot_global_wordcloud
from .report import build_report_markdown


def run_pipeline(
    input_jsonl: str = "data/tasks.jsonl",
    output_dir: str = "outputs",
    client = None,
    show_progress: bool = True
) -> dict:
    """è¿è¡Œå®Œæ•´çš„ 1vN ä»£ç è´¨é‡åˆ†æ Pipelineã€‚
    
    Args:
        input_jsonl: è¾“å…¥ä»»åŠ¡çš„ JSONL æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        client: OpenAI client å®ä¾‹
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
    
    Returns:
        åŒ…å«å„è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è¿è¡Œ 1vN ä»£ç è´¨é‡åˆ†æ Pipeline")
    print("=" * 60)
    
    # 1) è¯»å–ä»»åŠ¡
    print("\nğŸ“– [æ­¥éª¤ 1/6] è¯»å–ä»»åŠ¡æ•°æ®...")
    tasks = read_tasks_jsonl(input_jsonl)
    print(f"   âœ“ æˆåŠŸè¯»å– {len(tasks)} ä¸ªä»»åŠ¡")

    # 2) è°ƒç”¨ LLM åˆ†ææ¯ä»»åŠ¡
    print("\nğŸ¤– [æ­¥éª¤ 2/6] è°ƒç”¨ LLM åˆ†æä»»åŠ¡...")
    results = analyze_tasks(tasks, model=client, show_progress=show_progress)
    print(f"   âœ“ æˆåŠŸåˆ†æ {len(results)} ä¸ªä»»åŠ¡")

    # 3) å†™ per_task.jsonl
    print("\nğŸ’¾ [æ­¥éª¤ 3/6] ä¿å­˜ä»»åŠ¡åˆ†æç»“æœ...")
    ensure_dir(output_dir)
    per_task_path = os.path.join(output_dir, "per_task.jsonl")
    write_jsonl(per_task_path, results)
    print(f"   âœ“ å·²ä¿å­˜åˆ°: {per_task_path}")

    # 4) èšåˆå¯¼å‡º CSV
    print("\nğŸ“Š [æ­¥éª¤ 4/6] èšåˆç»Ÿè®¡æ•°æ®...")
    dim_csv = os.path.join(output_dir, "agg_dimension.csv")
    kw_csv = os.path.join(output_dir, "agg_keywords.csv")
    export_aggregates(per_task_path, dim_csv, kw_csv)
    print(f"   âœ“ ç»´åº¦ç»Ÿè®¡: {dim_csv}")
    print(f"   âœ“ å…³é”®è¯ç»Ÿè®¡: {kw_csv}")

    # 5) ç”Ÿæˆå…¨å±€å›¾è¡¨
    print("\nğŸ“ˆ [æ­¥éª¤ 5/6] ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    figs_dir = os.path.join(output_dir, "figs")
    ensure_dir(figs_dir)
    
    radar_path = os.path.join(figs_dir, "global_radar.png")
    plot_global_radar(dim_csv, radar_path)
    print(f"   âœ“ é›·è¾¾å›¾: {radar_path}")
    
    heatmap_path = plot_global_heatmaps(dim_csv, figs_dir)
    print(f"   âœ“ çƒ­åŠ›å›¾: {heatmap_path}")
    
    # è¯»å– FONT_PATH ç¯å¢ƒå˜é‡ä»¥æ”¯æŒä¸­æ–‡å­—ä½“
    font_path = os.environ.get("FONT_PATH")
    try:
        wordcloud_path = os.path.join(figs_dir, "global_wordcloud.png")
        plot_global_wordcloud(kw_csv, wordcloud_path, font_path=font_path)
        print(f"   âœ“ è¯äº‘å›¾: {wordcloud_path}")
    except Exception as e:
        print(f"   âš  è·³è¿‡è¯äº‘ç”Ÿæˆ: {e}")
        wordcloud_path = ""

    # 6) ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“ [æ­¥éª¤ 6/6] ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    report_md = os.path.join(output_dir, "report.md")
    build_report_markdown(dim_csv, kw_csv, figs_dir, report_md)
    print(f"   âœ“ æŠ¥å‘Š: {report_md}")

    print("\n" + "=" * 60)
    print("âœ… Pipeline æ‰§è¡Œå®Œæˆï¼")
    print("=" * 60)

    return {
        "per_task": per_task_path,
        "agg_dimension": dim_csv,
        "agg_keywords": kw_csv,
        "radar": radar_path,
        "heatmap": heatmap_path,
        "report": report_md,
        "wordcloud": wordcloud_path,
    }


def main() -> int:
    input_jsonl = os.environ.get("INPUT_JSONL", "/home/liangjunbiao/data/processed_data.jsonl")
    output_dir = os.environ.get("OUTPUT_DIR", "outputs")
    from openai import OpenAI

    client = OpenAI(
        base_url="http://localhost:8000/v1",
        api_key="EMPTY"  # vLLMé»˜è®¤ä¸éªŒè¯
    )

    paths = run_pipeline(input_jsonl=input_jsonl, output_dir=output_dir, client=client)
    for k, v in paths.items():
        print(f"{k}: {v}")
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
